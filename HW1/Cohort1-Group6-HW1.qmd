---
title: "Homework 1"
author: "Akash Ringe"
format: 
  pdf:
    include-in-header: 
      text: |
        \usepackage{bbm}
---

## Question 1:

**1.1**

```{r}
library(psych)
data <- read.csv("F-F_Research_Data_Factors_daily.csv")
data <- data[data$Date >= 20129999 & data$Date <= 20220831,]
mktrf_mean = mean(data$Mkt.RF, na.rm=TRUE)
mktrf_std = sd(data$Mkt.RF)
mktrf_skewness = skew(data$Mkt.RF)
mktrf_kurtosis = kurtosi(data$Mkt.RF)
mktrf_mean 
mktrf_std
mktrf_skewness
mktrf_kurtosis
```

#### 1.2

```{r}
```

#### 1.b) Write an `R` function to calculate the log-likelihood function from 1a above. Your function should take 3 arguments: (1) $\theta = (\beta, \sigma^2)$ a vector of all (four) parameters, (2) an $n \times k$ matrix $X$, and (3) a vector or $n \times 1$ matrix $y$.

```{r}
llf <- function(theta, X, Y) {
  beta <- theta[1:ncol(X)]
  sigma_square <- theta[ncol(X)+1]
  residual <- Y - X%*%beta
  return (nrow(Y) * log(2*pi)/2 + nrow(Y)*log(sigma_square)/2 + (t(residual)%*%residual)/(2*sigma_square))
}
```

#### 1.c) Use `optim()` and your function from 1b to find $\hat{\beta}_\text{MLE}$ and $\sigma^2_\text{MLE}$ as well as their standard errors. You may find it helpful to initialize your search at 0 for the $\beta$ parameters and $\text{var}(y)$ for $\sigma^2$.

```{r}
y <- matrix(ds$Salary)
x1 <- ds$Hits
x2 <- ds$Years
result <- optim(par = c(0,0,0,var(y)), fn=llf, hessian=T, Y=y, X=(cbind(1,x1, x2)))
exp_beta <- result$par
exp_std <- diag(solve(result$hessian))^0.5
exp_beta
exp_std
```

#### 1.d) Solve for the length-3 vector $\hat{\beta}_\text{MLE}$ and the scalar $\sigma^2_\text{MLE}$ (or use the derivations in the slides). Then analytically calculate $\hat{\beta}_\text{MLE}$ and $\sigma^2_\text{MLE}$ with the `Hitters` dataset. Compare your results to the output from `optim` in question 1c above.

```{r}
x = cbind(1, x1, x2)
XtX <- t(x) %*% x
XtY <- t(x) %*% y
invXtX <- solve(XtX)
beta_cap <- invXtX %*% XtY

y_cap <- x %*% beta_cap
residuals <- y - y_cap
SSR <- sum(residuals^2)
sigma_square <- SSR/nrow(x)

beta_cap
sigma_square
```

Both the results are equivalent.

#### 1.e) Test whether the 3 $\beta$ coefficients are each (separately) statistically significantly different from zero at a 95% confidence level.

```{r}
z <- (exp_beta[1:3] - 0)/exp_std[1:3]
z
```

Z-Scores indicate that the beta coefficients are statistically significant with 95% confidence.

\newpage

## Question 2: A Poisson Model for Count Data

Load the `trading_behavior` dataset.

The data provides 200 observations on equity trading behavior of Anderson students. `id` is an anonymized identifier for the student. `numtrades` is the median weekly number trades made by each student during the Fall quarter. `program` indicates whether the student is in the MSBA (1), MBA (2), or MFE (3) program (note that you may need to store this variable as a factor or convert it to a set of dummy variables when using it to fit a statistical model). `finlittest` is the students' scores on a financial literacy test taken before entering their graduate program (higher scores indicate higher financial "literacy").

Assume you want to model the number of trades as a function of graduate program (where $\mathbbm{1}$ is an indicator function) and financial literacy:

$$ y_i \sim \text{Pois}(\mu_i) $$

$$ \log \mu_i = \beta_0 + \beta_1\mathbbm{1}(MBA) + \beta_2\mathbbm{1}(MFE) + \beta_3 \text{finlittest} $$

```{r}
ds_anderson <- na.omit(read_csv("trading_behavior.csv"))
ds_anderson <- within(ds_anderson, {program = factor(program, levels = c("MSBA", "MFE", "MBA"))})
head(ds_anderson)
summary(ds_anderson)

y = matrix(ds_anderson$numtrades)
x1 = ds_anderson$program
x2 = ds_anderson$finlittest
x = cbind(1, x1, x2)
```

#### 2.a) A Poisson density for random variable $Y$ with parameter $\mu$ is $f(y|\mu) = \exp(-\mu)\mu^y/y!$. Suppose we let each $Y_i$ have it's own parameter $\mu_i$ with link function $\log(\cdot)$: specifically, $\log(\mu_i) = x_i'\beta$. Assume the data are sampled independently. Write, mathematically, the joint log-likelihood function.

$$
Likelihood :  L(\lambda, x_i) = \prod_{j=1}^nexp(-{\lambda})\frac1{x_j!}\lambda^{x_j}
$$

$$
Log \space Likelihood : l(\lambda, x_i) = -n\lambda - \sum_{j=1}^nln(x_j!)+ln(\lambda)\sum_{j=1}^nx_j
$$

#### 2.b) Write an `R` function to calculate the log-likelihood function from 2a above. Your function should take 3 arguments: (1) a vector $\beta$ of all (four) parameters, (2) an $n \times k$ matrix $X$, and (3) a vector or $n \times 1$ matrix $y$.

```{r}
llf_poisson <- function(theta, mat_X, mat_y) {
  beta <- theta[1:ncol(mat_X)]
  lamda <- mean(exp(mat_X%*%beta))
  op <- -nrow(mat_y)*lamda - sum(log(factorial(mat_y))) + log(lamda)*sum(mat_y)
  return (-op)
}
```

#### 2.c) Use `optim()` and your function from 1b to find $\hat{\beta}_\text{MLE}$ as well as their standard errors.

```{r}
result = optim(par = c(0,0,0,0), fn=llf_poisson, hessian=T, mat_X=x, mat_y=y)
result
```

#### 2.d) Fit the model using `glm()`. Compare your results to the output from `optim` in question 2c above.

The results are the same.

#### 2.e) The "analog" to the F-test from linear regression is the Likelihood Ratio Test. The Likelihood Ratio test statistic is calculated as:

$$ LR_n = 2 \times [ \ell_n(\hat{\theta}) - \ell_n(\tilde{\theta})] $$

#### where $\ell_n(\cdot)$ is the log likelihood function, $\hat{\theta}$ is the MLE, and $\tilde{\theta}$ is a constrained parameter vector (e.g., suppose a Null Hypothesis is that $\theta_2=0$ & $\theta_3=0$). The Likelihood Ratio test statistic $LR_n$ has an asymptotic chi-squared distribution with $k$ degrees of freedom (i.e., $\chi^2_k$ where $k$ is the length of the $\theta$ vector).

#### Test the joint hypothesis that $\beta_2=0$ & $\beta_3=0$ at the 95% confidence level using a Likelihood Ratio test. Specifically, use your log-likelihood function from 2b above and your parameter estimates from 2c above to calculate $\ell_n(\hat{\beta_\text{MLE}})$. Then replace $\beta_2$ and $\beta_3$ with their hypothesized values and re-calculate the log-likehood (ie, $\ell_n([\hat{\theta}_1,0,0,\hat{\theta}_2])$. Next, compute $LR_n$ and compare the value to the cut-off of a chi-squared distribution with 4 degrees of freedom to assess whether or not you reject the Null Hypothesis.

```{r}
lrt <- function(theta_cap, theta_constr) {
  l_ratio <- 2*(llf_poisson(theta_cap, x, y) - llf_poisson(theta_constr, x, y))
  return (l_ratio)
}

theta_cap <- result$par
theta_constr <- c(theta_cap[1], 0, 0, theta_cap[4])
lrt(theta_cap, theta_constr)
llf_poisson(theta_cap, x, y)
llf_poisson(theta_constr, x, y)
```

\newpage

## Question 3: Estimating Demand via the Multi-Nomial Logit Model (MNL)

Suppose you have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 4 products, then either $y=3$ or $y=(0,0,1,0)$ depending on how you want to represent it. Suppose also that you have a vector of data on each product $x_j$ (eg, size, price, etc.).

The MNL model posits that the probability that consumer $i$ chooses product $j$ is:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 4 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta} + e^{x_4'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=\delta_{i4}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 \times \mathbb{P}_i(4)^0 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$

Use the `yogurt_data` dataset, which provides the anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were "featured" in the store as a form of advertising (`f1`:`f4`), and the products' prices (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase. Consumers 2 through 7 each bought yogurt 2, etc.

Let the vector of product features include brand dummy variables for yogurts 1-3 (omit a dummy for product 4 to avoid multi-collinearity), a dummy variable to indicate featured, and a continuous variable for price:

$$ x_j' = [\mathbbm{1}(\text{Yogurt 1}), \mathbbm{1}(\text{Yogurt 2}), \mathbbm{1}(\text{Yogurt 3}), X_f, X_p] $$

You will need to create the product dummies. The variables for featured and price are included in the dataset. The "hard part" of this likelihood function is organizing the data.

Your task: Code up the log-likelihood function. Use `optim()` to find the MLEs for the 5 parameters ($\beta_1, \beta_2, \beta_3, \beta_f, \beta_p$).

(Hint: you should find 2 positive and 1 negative product intercepts, a small positive coefficient estimate for featured, and a large negative coefficient estimate for price.)

```{r}
library(dplyr)
ds_yogurt=read.csv('yogurt_data.csv')

x=cbind(ds_yogurt['id'],stack(select(ds_yogurt,contains('y'))),
        stack(select(ds_yogurt,contains('f'))),
        stack(select(ds_yogurt,contains('p'))))
colnames(x)=c('cos_i','delta_ij','prod_j','is_ad','ad_j','price','p_j')
x[c('is_y1','is_y2','is_y3')]=0
x['is_y1'][x['prod_j']=='y1']=1
x['is_y2'][x['prod_j']=='y2']=1
x['is_y3'][x['prod_j']=='y3']=1

y=x['delta_ij']

mnl_func=function(beta,x){
    prob_sum=0

    for (i in 1:dim(unique(x['cos_i']))[1]){

      costu=filter(x,cos_i==i)
      
      deno=0
      for (j in 1:dim(costu)[1]){
        deno=deno+exp(as.matrix(costu[c('is_y1','is_y2','is_y3','is_ad','price')][j,])%*%beta)[1]}
      
      prob_i=0
      for (j in 1:dim(costu)[1]){
        prob_i=prob_i+
          log(exp(as.matrix(costu[c('is_y1','is_y2','is_y3','is_ad','price')][j,])
                  %*%beta)[1]/deno)*costu$delta_ij[j]}
      
      prob_sum=prob_sum+prob_i
    }
  return(prob_sum)
}

out=optim(par=c(0,0,0,0,0),fn=mnl_func,x=x,hessian=TRUE,control=list(fnscale=-1))
out$par
```
